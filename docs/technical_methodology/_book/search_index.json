[["index.html", "Economics Methodology 1 Welcome!", " Economics Methodology Lorae Stojanovic 2024-07-22 1 Welcome! The purpose of this document is to explain, in depth, how the FIM works. It is intended to encapsulate the technical elements of the project arising from the programming design as well as the economic design. Chapter 1 documents the sources of data. Chapter 2 documents how the FIM is calculated. Chapter 3 documents how the UI interface is constructed. Chapter 4 describes the economic intuition for the FIM. "],["data-sources.html", "2 Data Sources 2.1 Data sources 2.2 Code walk-through 2.3 Combining &amp; wrangling 2.4 The clean data", " 2 Data Sources 2.1 Data sources We will have to figure this section out. We draw from BEA, CBO, haver, etc. But we will need a flow chart of all the scripts, all the dependencies, and explain why each source is used for each item. When do we overwrite history? We’ll probably have one sub section for each source, like: The FIM draws from several sources of data each time it runs. It uses the BEA’s National Accounts data, accessed through Haver, to get history. It uses our own predictions for the forecast period. And it uses CBO for past and future potential GDP and GDP predictions. 2.2 Code walk-through Let’s get nitty-gritty into the code. When the FIM runs, it starts with some messy initialization. It loads packages and defines some variables related to dates. This code should be made self-contained and put into its own script. But I (Lorae) haven’t gotten to that yet. Note that this code is important and should be handled carefully. For example, variables like last_month_year and month_year are used several times in code that follows. When refactoring this section, tread lightly and test carefully. Sys.setenv(TZ = &#39;UTC&#39;) # Set the default time zone to UTC (Coordinated Universal Time) # Load packages librarian::shelf( tidyverse, tsibble, lubridate, glue, TimTeaFan/dplyover, zoo, TTR, fs, gt, openxlsx, snakecase, rlang, BrookingsInstitution/ggbrookings ) # Load all functions in package (?!?) devtools::load_all() options(digits = 4) # Limit number of digits options(scipen = 20)# Turn off scientific notation under 20 digits #are we running this after a cbo baseline and pre-bea update? post_cbo_baseline&lt;- FALSE # Set the value of &#39;month_year&#39; to the current month and year (in the format &quot;mm-yyyy&quot;) month_year &lt;- glue(&#39;{format.Date(today() - 7, &quot;%m&quot;)}-{year(today())}&#39;) print(month_year) # Calculate the current date minus 7 days current_date &lt;- today() - dweeks(1) # Calculate the previous month date, handling wraparound (i.e. previous # month to January (&quot;01&quot;) is December (&quot;12&quot;), not month &quot;0&quot;) last_month_date &lt;- current_date %m-% months(1) # Extract and format the month as a two-digit string last_month_2digit &lt;- sprintf(&quot;%02d&quot;, month(last_month_date)) # Extract the year from the last_month_date last_year &lt;- year(last_month_date) # Create last_month_year string for file naming last_month_year &lt;- glue(&#39;{last_month_2digit}-{last_year}&#39;) print(last_month_year) This next section of code primarily serves to define empty folders that will be populated upon running the FIM. This section could also use a clean-up, but it relies on some variables produced in the previous code chunk. Perhaps the two sections should be bundled together upon refactoring. #setting our reference period to be the post-cbo files if we&#39;ve already produced # fim output incorporating the cbo update if(file.exists(glue(&#39;results/{month_year}-post-cbo&#39;))){ last_month_year&lt;- glue(&#39;{month_year}-post-cbo&#39;) } # Create folder for current update in the results directory dir_create(glue(&#39;results/{month_year}&#39;)) # Folder to store forecast sheet from current update dir_create(glue(&#39;results/{month_year}/input_data&#39;)) # Beta folder for Lorae&#39;s refactored results dir_create(glue(&#39;results/{month_year}/beta&#39;)) # Copy the file &#39;forecast.xlsx&#39; from the &#39;data&#39; directory to the &#39;input_data&#39; directory # This is the copy we keep for the current update file_copy( path = &#39;data/forecast.xlsx&#39;, new_path = glue(&#39;results/{month_year}/input_data/forecast_{month_year}.xlsx&#39;), overwrite = TRUE ) This next section reads in raw data. I’ve taken pains to attempt to trace back where the inputs in this project arise. As you can see, there are two crucial .rds files saved in the FIM. I’m not exactly sure how they are overwritten, and understanding where they are replaced is a crucial part of the FIM restructuring process. After investigating, I’ve found that these two data files are overwritten by the Haver pull script. # Load in national accounts. This file is rewritten each time data-raw/haver-pull.R # is run. fim::national_accounts # this is the literal df load(&quot;data/national_accounts.rda&quot;) # this loads in a df named national_accounts # Load in projections. This file is rewritten each time data-raw/haver-pull.R # is run. fim::projections # this is the literal df load(&quot;data/projections.rda&quot;) # this loads in a df named projections This section reads in historical overrides. Historical overrides are data observations in the past that we take as given. Every time this code runs, it extracts raw historic data from Haver and attempts to use that. However, for some time series, we do not want to use what Haver gives us. Instead, we override that data. These overrides are crucial to that process. If we have to override each time, why do we even bother reading in the raw data? It’s history, right? Well, not really. The data series we use as inputs in the FIM occasionally get revised. We want that most recent revision, because it’s most accurate. However, we want to make sure that these revisions don’t overwrite our historical overrides. # Read in historical overrides from data/forecast.xlsx # Since BEA put all CARES act grants to S&amp;L in Q2 2020 we need to # override the historical data and spread it out based on our best guess # for when the money was spent. historical_overrides &lt;- readxl::read_xlsx(&#39;data/forecast.xlsx&#39;, sheet = &#39;historical overrides&#39;) %&gt;% # Read in historical_overrides select(-name) %&gt;% # Remove longer name since we don&#39;t need it pivot_longer(-variable, names_to = &#39;date&#39;) %&gt;% # Reshape so that variables are columns and dates are rows pivot_wider(names_from = &#39;variable&#39;, values_from = &#39;value&#39;) %&gt;% mutate(date = yearquarter(date)) # Read in deflator overrides from data/forecast.xlsx deflator_overrides &lt;- readxl::read_xlsx(&#39;data/forecast.xlsx&#39;, sheet = &#39;deflators_override&#39;) %&gt;% # Read in overrides for deflators select(-name) %&gt;% # Remove longer name since we don&#39;t need it pivot_longer(-variable, names_to = &#39;date&#39;) %&gt;% # Reshape so that variables are columns and dates are rows pivot_wider(names_from = &#39;variable&#39;, values_from = &#39;value&#39;) %&gt;% mutate(date = yearquarter(date)) This next code chunk seriously needs attention. We certainly should not be setting the current quarter this far down in the code and intermingling it with our data pipeline. It should be somewhere in the top of the code. Why haven’t I changed it yet? Well, it relies on the data frames we load. I haven’t had time to come up with a better way of doing it. But this one line of code needs serious attention. # TODO: This current quarter should be calculated at the top, for the entirety # of the FIM, not buried down here. # Save current quarter for later current_quarter &lt;- historical_overrides %&gt;% slice_max(date) %&gt;% pull(date) This next section imports data from the projections dataframe, which - as we covered above - is saved in data/projections.rda. The data frame initially comes with 30 variables. In the subsequent lines of code, we only keep 15 of them. We then use the real and nominal values of federal purchases, state purchases, and consumption to produce deflators. After calculating the deflators, we calculate growth rates on those three variables as well as GDP, real GDP, and real potential GDP. We then delete the intermediate variables that are no longer needed. projections &lt;- projections %&gt;% # Rename the variables from their Haver codes transmute( id, date, state_ui, federal_ui, gdp, real_gdp = gdph, real_potential_gdp = gdppothq, gdp_deflator = jgdp, consumption = c, real_consumption = ch, real_federal_purchases = gfh, real_state_purchases = gsh, federal_purchases = gf, state_purchases = gs, unemployment_rate) projections &lt;- projections %&gt;% # Implicit price deflators mutate( # Why do we calculate these values and not get them from Haver instead? federal_purchases_deflator = federal_purchases/real_federal_purchases, state_purchases_deflator = state_purchases/real_state_purchases, consumption_deflator = consumption/real_consumption ) %&gt;% # Growth rates mutate( across( .cols = c( &quot;gdp&quot;, &quot;real_gdp&quot;, &quot;real_potential_gdp&quot;, &quot;federal_purchases_deflator&quot;, &quot;state_purchases_deflator&quot;, &quot;consumption_deflator&quot;), #&quot;jgse&quot;), # Calculate quarterly growth rate using qgr() function, equal to x/lag(x), # then subtract 1. .fns = ~ qgr(.) - 1, .names = &quot;{.col}_growth&quot; ) ) %&gt;% # Turn date into time series mutate(date = tsibble::yearquarter(date)) %&gt;% # reorder the id column before the date column relocate(id, .before = date) %&gt;% # convert the projections df into a tsibble data frame type tsibble::as_tsibble(key = id, index = date) %&gt;% # TODO: as you can see from the select function, many columns are not kept. # Perhaps the code can be refactored to exclude the data processing steps # in the first place. select( -real_federal_purchases, # we don&#39;t need anymore, as we created the _deflator_growth var already -real_state_purchases, # we don&#39;t need anymore, as we created the _deflator_growth var already -federal_purchases, # we don&#39;t need anymore, as we created the _deflator_growth var already -state_purchases, # we don&#39;t need anymore, as we created the _deflator_growth var already -federal_purchases_deflator, # we don&#39;t need anymore, as we created the _deflator_growth var already -state_purchases_deflator, # we don&#39;t need anymore, as we created the _deflator_growth var already -consumption_deflator # we don&#39;t need anymore, as we created the _deflator_growth var already ) This section imports data from the national_accounts dataframe, which - as we covered above - is saved in data/national_accounts.rda. The data frame initially comes with 90 variables. In the subsequent lines of code, we only keep 51 of them. TODO: I actually think more of the variables can be deleted later. national_accounts &lt;- national_accounts %&gt;% # Let&#39;s rename these 90 variables to something we can understand transmute( id, date, gdp, real_gdp = gdph, gdp_deflator = jgdp, consumption = c, real_consumption = ch, consumption_deflator = jc, federal_purchases_deflator = jgf, state_purchases_deflator = jgs, consumption_grants_deflator = jgse, investment_grants_deflator = jgsi, medicare = yptmr, medicaid = yptmd, ui = yptu, social_benefits = gtfp, personal_taxes = yptx, corporate_taxes = yctlg, purchases = g, federal_purchases = gf, state_purchases = gs, real_federal_purchases = gfh, real_state_purchases = gsh, federal_personal_taxes = gfrpt, federal_production_taxes = gfrpri, federal_corporate_taxes = gfrcp, federal_payroll_taxes = gfrs, federal_social_benefits = gftfp, gross_consumption_grants = gfeg, state_personal_taxes = gsrpt, state_production_taxes = gsrpri, state_corporate_taxes = gsrcp, state_payroll_taxes = gsrs, state_social_benefits = gstfp, health_grants = gfeghhx, medicaid_grants = gfeghdx, investment_grants = gfeigx, federal_subsidies = gfsub, state_subsidies = gssub, rebate_checks = gftfpe, nonprofit_provider_relief_fund = gftfpv, ui_expansion = gftfpu, wages_lost_assistance = coalesce(yptol, 0), # idk what this does real_potential_gdp = gdppothq, recession = recessq, gdp_deflator_growth = jgdp_growth, consumption_deflator_growth = jc_growth, federal_purchases_deflator_growth = jgf_growth, state_purchases_deflator_growth = jgs_growth, consumption_grants_deflator_growth = jgse_growth, investment_grants_deflator_growth = jgsi_growth ) After importing the variables we need, we merge the projections data frame and the national_accounts data frames using the coalesce_join user defined function. With national_accounts assigned to the x argument and projections assigned to the y argument, national_accounts takes precedence over projections if there are any conflicting data entries. The resultant data frame, usna1, is an assortment of XXX variables. usna1 &lt;- coalesce_join(x = national_accounts, y = projections, by = &#39;date&#39;) %&gt;% as_tsibble(key = id, index = date) —- end of code walk through, for now! We’ll probably have one sub section for each source, like: 2.2.1 CBO stuff here. Which reports do we use? Budget? Economic? How do we input their data into the FIM? 2.2.2 BEA Same questions as above 2.2.3 Haver Discuss Haver package, etc. 2.3 Combining &amp; wrangling I have no idea how this is done, and we will have to refactor this section in depth. 2.4 The clean data We use the “projections” data frame (at least for now) in the remainder of the code. In its current refactored state in the fiscal_impact_BETA script, we select columns of interest from the lonnngggg “projections” data frame and use those as we calculate the FIM. "],["data-processing.html", "3 Data Processing 3.1 Input data 3.2 The equations that actually fovern the FIM", " 3 Data Processing 3.1 Input data The FIM has many inputs. But when we run the FIM, we typically take into account 23 main data series, which are found in the “forecast” section of the sheet. “An image of the ‘forecast’ page of the ‘forecast.xlsx’ workbook.” 3.2 The equations that actually fovern the FIM Your best source for data processing steps as they stand (in late June 2024) is to look at docs/variables_tracker. It maps out how each input leads to each output and what operations are done along the way. I’m using it to completely refactor the computation section of the FIM. We’ll have to write up this section later. 3.2.1 MPC subsection There’s a lot of cool documentation about how we calculate MPCs using matrices in the src/mpc_lorae.R file. Please refer to that to write about how the economic concept of MPCs translates into code. 3.2.2 minus neutral subsection I’m sure we’ll have stuff about our reasoning here 3.2.3 other stuff section more content "],["ui-presentation.html", "4 UI Presentation", " 4 UI Presentation This chapter will focus on how the results are presented in a user-friendly UI. We will use various R packages such as shiny to create interactive dashboards. "],["economics-methodology.html", "5 Economics Methodology", " 5 Economics Methodology In this chapter, we will delve into the economics behind the FIM, including the theoretical foundations and the specific methodologies used to calculate various economic impacts. 5.0.1 Marginal Propensities to Consume 5.0.1.1 Calculation Explain how the marginal propensities to consume are calculated using matrices and provide relevant examples. # Example code for calculating MPC series &lt;- c(100, 200, 300, 400) 1+1 ## [1] 2 testing "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
